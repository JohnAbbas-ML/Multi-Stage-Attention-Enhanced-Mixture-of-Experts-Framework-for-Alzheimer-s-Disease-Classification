# Alzheimer Mixture of Experts (MoE) with Channel & Spatial Attention

This repository provides a PyTorch implementation of an **Alzheimerâ€™s Disease classification model** based on a **Mixture of Experts (MoE)** architecture with **Channel and Spatial Attention**.
The model integrates DenseNet-inspired feature extraction, a gating mechanism for expert selection, and attention modules to improve interpretability and performance on medical imaging tasks.

---

## ğŸš€ Features

* **Dense Blocks** â€“ Efficient feature reuse with dense connections.
* **Channel Attention (CA)** â€“ Enhances informative feature channels.
* **Spatial Attention (SA)** â€“ Focuses on key spatial regions in MRI/CT scans.
* **Mixture of Experts (MoE)** â€“ Multiple experts with adaptive gating network.
* **Auxiliary Classifier** â€“ Helps stabilize training with intermediate supervision.
* **Multi-output** â€“ Provides main and auxiliary predictions along with attention maps.

## ğŸ§  Model Overview

The model consists of:

1. **Initial Convolution Layer** â€“ Standard stem for feature extraction.
2. **MoE Stages (3x)** â€“ Each stage has:

   * Multiple **Experts** (DenseBlocks + Attention).
   * **Gating Network** to assign weights to experts.
   * **Transition Layer** for downsampling.
3. **Auxiliary Classifier** â€“ Early classification branch.
4. **Main Classifier** â€“ Final prediction head.

---

## â–¶ï¸ Usage

Run the model script to print the architecture and test a forward pass:

```bash
python alzheimer-moe.py
```

Example output:

```
===== Model Architecture =====
... (torchsummary output)

===== Forward Pass Test =====
Main output shape: torch.Size([1, 3])
Auxiliary output shape: torch.Size([1, 3])
```

---

## ğŸ“Š Outputs

The model returns:

* **Main Output** â†’ Final classification logits.
* **Auxiliary Output** â†’ Intermediate classification logits.
* **Gating Weights** (optional) â†’ Expert assignment probabilities.
* **Attention Maps** (optional) â†’ Channel/Spatial attention visualization.

Example forward pass:

```python
from model import AlzheimerMoEModel
import torch

model = AlzheimerMoEModel(input_channels=3, num_classes=3)
dummy_input = torch.randn(1, 3, 224, 224)

main_out, aux_out, gates, attn = model(dummy_input, return_attention=True)
```

---
